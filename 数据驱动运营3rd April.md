## 大数据思维

2011和2012年大数据概念活了之后，传统行业也好、互联网企业也好，都把自己的业务网大数据靠一靠。
【案例1：输入法】【案例2：地图】
大数据思维对应的是数据驱动，而传统决策方式有：1. 因果驱动；2.拍脑袋
那我们从上面的两个案例来看一下，纸质的地图和新的手机地图之间，智能ABC输入法和搜狗输入法之间都有什么区别？
这里最大的差异就是__有没有用上新的数据__。这里由此引出了一个概念 -  **数据驱动**。有了这些数据，基于数据上的统计也好，做其他挖掘也好，把一个产品做的更加智能，变得更加好，这个跟它对应的就是之前没有数据的情况，可能是拍脑袋的方式，或者说我们用过去的经验，我们想清楚为什么然后再去做这个事情。

但是，如果等待数据的时间周期很长，可能整个时机就错过了，那么有些人最后还是干脆拍脑袋，就不等待这个数据了，这样的过程效率是非常低的，并不是说最终拿不到这个数据，只是等待的时间太长，而**效率低的情况下我们错过了很多机会。**

光有数据仪表盘是不够的（老板当然还是高兴的），对于市场和运营的同学来说就差的很远。那这个时候我们需要自助式数据分析（Self-service Data Analytics）,让参与业务的人真正掌握数据，去进行数据分析。

![alt text](https://github.com/bakerX/Diary/blob/master/images/self-service-data.jpg)

这里对应的就是一个很强烈的对比，我们源头是一堆杂乱的数据，中间有一个工程师用来跑这个数据，然后右边是接各种业务部门的需求，然后排队等待被处理，这种方式是非常低效和无序的。而理想的好的状态是，我们先将大数据源本身整好，整全整细，中间提供强大的分析工具，让每一个业务员都能直接进行操作，大家并发的去做一些业务上的数据需求，这样效率就高了。

**下面来讲数据处理的流程：**

![alt text](https://github.com/bakerX/Diary/blob/master/images/Bigdata-pyramid.jpg)

__1. 数据采集__
基本原则
..* 全：多种源（客户端、服务端、数据库等），全量而非抽样
..* 细：Who、When、Where、How、What

数据这个事情，如果想要做好，最重要的就是数据源。用一个好的查询引擎、一个慢的查询引擎无非是时间上消耗不一样，单数数据源如果差的话，后面再复杂的算法都解决不了这个问题，也难以得到正确的结论。

__2. 数据建模__

有了数据之后，就要对数据进行加工，不能把原始的数据直接暴露给上面的业务分析人员，它可能本身是杂乱的，没有经过很好的逻辑抽象。
这里就会涉及到数据建模。首先，提一个概念就是数据模型。许多人对数据模型这个词产生一种畏惧感，觉得模型这个东西很高深，很复杂，但其实这个事情非常简单。

![alt text](https://github.com/bakerX/Diary/blob/master/images/bigdata-model.jpg)

这是神测数据的CEO桑文锋讲述的他自己在家里干过的一件事情：我自己家里的家谱在文革的时候被烧掉了，后来家里的长辈说一定要把家谱这些东西给存档一下，因为我会电脑，就帮着用电脑去处理了一下这些家族的数据这些关系，整理出了族谱。

我们现实是一个个的人，家谱里面的人，通过一个树形的结构，还有他们之间的数据关系，就能把现实实体的东西用几个简单图给标示出来，这里就是一个数据模型。

数据模型是对现实世界的一个抽象化的数据表示。创业公司经常是这么一个情况，当面对业务时候，一般前端做一个请求，然后对请求经过处理，再更新到数据库里面去，数据库里面建了一系列的数据表，数据表之间都是很多的依赖关系。

![alt text](https://github.com/bakerX/Diary/blob/master/images/bigdata-relation.jpg)

就比如上面这张图片里面展示的那样，一个业务线发展差不多一年以上就可能牵扯到几十张甚至上百张数据表，然后这个表直接提供给业务分析人员去使用，理解起来难度非常大的。
**这个数据模型是用于满足你正常的业务运转**，为产品正常的运行而建的一个数据模型。但是它并不是一个针对分析人员使用的模型。如果，非要把它用于数据分析那就带来了很多问题。比如：它理解起来非常麻烦。

另外，数据分析很依赖表之间的这种格式，比如：某一天我们为了提升性能，对某一表进行了拆分，或者加了字段、删了某个字段，这个调整都会影响到你分析的逻辑。

![alt text](https://github.com/bakerX/Diary/blob/master/images/bigdata-model2.jpg)

这里，最好要针对分析的需求对数据重新进行建模，它可能内容是一只的，但是我们的组织方式变了一下。就拿用户行为这块数据来说，就可以对它进行一个抽象，然后把它作为一个分析表。

用户在产品上进行的一系列的操作，比如浏览一个商品，然后谁浏览的，什么时间浏览的，他用的什么操作系统，用的什么浏览器的版本，还有他这个操作看了什么商品，这个商品的一些属性是什么，这个东西都给他进行了一个很好的抽象。这种抽样的很大好处就是很容易理解，看过去一眼就知道这表示什么，对分析来说也更加方便。

![alt text](https://github.com/bakerX/Diary/blob/master/images/bigdata-model3.jpg)

在数据分析领域，特别是针对用户行为分析方面，目前比较有效的一个模型就是**多维数据模型**，“在线分析处理”这个模型。它里面有这个关键的概念，**一个是维度，一个是指标。**

维度就是一个属性。比如“城市”是一个维度，它有“北京”、“天津”、“上海”这些取值。“操作系统”也是一个维度，有“Mac OS”、“OS”、“Android”这些取值。

通过维度交叉，就可以看一些指标情况，比如用户量、销售额，这些就是指标。比如，通过这个模型就可以看来自北京，使用iOS的，他们的整体销售额是怎么样的。

这里只是举了两个维度，可能还有很多个维度。总之，通过维度组合就可以看一些指标的数，大家可以回忆一下，大家常用的这些业务的数据分析需求是不是许多都能通过这种简单的模式给抽样出来。


__3. 数据分析方法__

* 多维事件分析
* 漏斗分析
* 留存分析
* 回访分析

对于互联网产品日常的用户行为分析来说，有这么四种：

* 一种是多维事件分析，分析维度之间的组合、关系
* 第二种是漏斗分析，对于电商、订单相关的这种行为的产品来说非常重要，要看不同的渠道转化这些东西。
* 第三种是留存分析，用户来了之后我们希望他不断的来，不断的进行购买，这就是留存。
* 第四种回访，回访是留存的一种特别的形式，可以看他一段时间访问的频次，或者访问的时间段的情况。

**【方法1：多维事件分析法】

首先来看多维事件的分析，这块常见的运营、产品改进这种效果分析。其实，大部分情况都是能用多维事件分析，然后对它进行一个数据上的统计。
**1. 【三个关键概念】

![alt text](https://github.com/bakerX/Diary/blob/master/images/data-analysis.jpg)

这里其实就是由三个关键的概念，一个就是事件，一个是维度，一个是指标。

**事件**就是说任何一个互联网产品，都可以把它抽象成一系列事件，比如针对电商产品来说，可抽象到提交订单、注册、收到商品一系列时间用户行为。

每一个事件里面都包括一系列属性。比如，他用的操作系统版本、是否连接Wi-Fi；比如，订单相关的运费，订单总价这些东西，或者用户的一些职能属性，这就就是**一系列维度。**

**基于这些维度看一些指标的情况。**比如，对于提交订单来说，可能是他总提交订单的次数做成一个指标，提交订单的人数是一个指标，平均的人均次数也是一个指标；订单的总和、总价这些都是一个指标，运费这也是一个指标，统计一个数后就能把它抽样成一个指标。

**2. 【多维分析的价值】

来看一个例子，看看多维分析它的价值。

![alt text](https://github.com/bakerX/Diary/blob/master/images/data-analysis2.jpg)

比如，对于订单支付这个事件来说，针对整个总的成交额这条曲线，按照时间的曲线会发现它一路在下跌。但下跌的时候，不能眼睁睁的看着它，一定要分析原因。

怎么分析这个原因呢？常用的方式就是对**维度进行一个拆解**，可以按照某些维度进行拆分，比如我们按照地域，或者按照渠道，或者按照其他一些方式去拆开，按照年龄段、按照性别去拆开，看这些数据到底是不是整体在下跌，还是说某一类数据在下跌 。

![alt text](https://github.com/bakerX/Diary/blob/master/images/data-analysis3.jpg)

这是一个假想的例子 - 按照支付方式进行拆开之后，支付方式有三种，有用支付宝Alipay，或者用微信支付，或者用银行卡支付这三种方式。

通过数据可以看到支付宝、银行支付基本是一个沉稳的一个状态。但是，如果看微信支付，会发现从最开始最多，一路下跌到非常少，通过这个分析就知道微信这种支付方式，肯定存在某些问题。

比如：是不是升级了这个接口或者微信本身出了什么问题，导致它流量下去了？

**【方法2：漏斗分析】

漏斗分析是用数据来看一个用户从做第一步操作到后面每一步操作，实际的转化过程。

![alt text](https://github.com/bakerX/Diary/blob/master/images/data-analysis4.jpg)

比如，一批用户先浏览了你的首页，浏览首页之后可能一部份人就直接先跑了，还有一部分热那可能去点击到另一个商品里面，点击到商品的可能又有很多人跑了，接下来可能就有一部分人直接就购买了，这其实就是一个漏斗。

![alt text](https://github.com/bakerX/Diary/blob/master/images/data-analysis5.jpg)

通过这个漏斗，就能分析一步一步的转化情况，然后每一步都有流失，可以分析不同的渠道其转化情况如何。比如，打广告的时候发现来自百度的用户漏斗转化效果比较好，就可能在广告投放上就在百度上多投一些。

【方法3：留存分析】

比如，搞了一个地推活动，然后来了一批注册用户，接下来看它的关键行为上面操作的特征，比如当天它有操作，第二天有多少人会关键操作，第N天有多少操作，这就是看它留下来这个情况。

【方法4：回访分析】

![alt text](https://github.com/bakerX/Diary/blob/master/images/data-analysis6.jpg)

回访就是看进行某个行为的一些频度特征，对于购买黄金这个行为来说，在一周之内至少有一天购买黄金的人有多少，至少有两天的有多少人，至少有七天的有多少人，或者说购买多少次数这么一个分布，就是回访回购这方面的分析




一个典型的数据平台可以分为数据接入、数据传输、数据存储和建模、数据统计分析与挖掘、数据的可视化和最终反馈。
一个典型的互联网用户产品通常会接入三类数据：

![alt text](https://github.com/bakerX/Diary/blob/master/images/Bigdata1.jpg)

其中接入最多和使用最广泛的一类是前端操作数据。还有更广泛的数据就是后端日志。还有一类就是业务数据，也可能是第三方服务，如客服、配送等通过数据接口传递来的一些信息。目前对于业务数据的分析，是缺乏现成工具的，一般都是需要临时性的写SQL来分析，这件事情非常繁杂、细碎和痛苦。数据接入是一件苦事。一方面，数据的适用方和接入方通常会是两个团队，数据的使用方是产品或者运营等非技术同学，数据的接入方则是技术人员，所以每次要做数据接入，都会有冗长而琐碎的沟通；二，技术数据使用方搞定了技术帮你添加新的埋点，这也会是一个很漫长的过程，要先开发，然后发版，然后等待用户更新，整个流程很长，迭代代价很痛苦。并且其中会由很多难以把握的过程，例如，一个简单的代码错误，一个简单的调用错误，都无法保证数据的完整性和正确性；三，我们的企业，我们的业务会持续发展，运营和推广活动也会越来越频繁，我们会发现，在前端埋的点会越来越多，这些埋点会多到让人无法管理，让人抓狂的程度。

下面是神测数据的全端数据接入：

![alt text](https://github.com/bakerX/Diary/blob/master/images/Bigdata2.jpg)

完成了数据的接入，获取了全端数据，就可以迅速地用前端可视化来看下效果。那么数据分析能做什么？
典型的数据分析需求有下面这些，比如运营监控＼产品改进和商业决策等等，这些都是基础需求（10%的数据问题）。

![alt text](https://github.com/bakerX/Diary/blob/master/images/Bigdata3.jpg)
